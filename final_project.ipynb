{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import your modules needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from transformers import pipeline, logging\n",
    "import numpy as np\n",
    "from langchain_openai import OpenAI\n",
    "import random\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "SEC_API_KEY = os.getenv('SEC_API_KEY')\n",
    "OPENAI_KEY = os.getenv('OPENAI_API_KEY')\n",
    "FMP_KEY = os.getenv('FMP_API_KEY')\n",
    "EMAIL=os.getenv(\"EMAIL\")\n",
    "\n",
    "sleep_time = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instantiate an OpanAI LLM object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = OpenAI(openai_api_key=OPENAI_KEY, temperature=0.9)\n",
    "llm = OpenAI(openai_api_key=OPENAI_KEY, temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate ProsusAI/Finbert model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence transformers logging (optional)\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Load FinBERT sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prompt the user to get company name to retrieve ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get company ticker by the provided name\n",
    "def get_ticker_by_company_name(company_name):\n",
    "    url = \"https://financialmodelingprep.com/api/v3/search\"\n",
    "    params = {\n",
    "        \"query\": company_name,\n",
    "        \"limit\": 1,\n",
    "        \"exchange\": \"NASDAQ\",\n",
    "        \"apikey\": FMP_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = req.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        results = response.json()\n",
    "        if results:\n",
    "            return results[0][\"symbol\"]\n",
    "        else:\n",
    "            print(f\"No ticker found for: {company_name}, using default tickers.\")\n",
    "            default_tickers = ['UPST', 'KO', 'TSLA', 'INTC']\n",
    "            return default_tickers\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching ticker: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names = input(\"Enter the company name, use , to separate multiple inputs: \")\n",
    "print(f\"Company name: {company_names}\")\n",
    "company_names = company_names.split(\",\")\n",
    "print(f\"Company names: {company_names}\")\n",
    "company_tickers = [get_ticker_by_company_name(name.strip()) for name in company_names]\n",
    "print(f\"Ticker for '{company_names}' is: {company_tickers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use edgar api to search company using their ticker to fetch FR document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comp_sec(ticker):\n",
    "    \"\"\"\n",
    "    Get the company SECURITIES AND EXCHANGE COMMISSION reports for a given ticker symbol.\n",
    "    \n",
    "    Args:\n",
    "        ticker (str): The ticker symbol of the company.\n",
    "    \n",
    "    Returns:\n",
    "        object: filings of the company.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.sec-api.io\"\n",
    "    payload = {\n",
    "                \"query\": f'formType:\\\"10-K\\\" AND ticker:{ticker} AND filedAt:[2020-01-01 TO 2025-01-31]',\n",
    "                \"from\": \"0\",\n",
    "                \"size\": \"50\",\n",
    "                \"sort\": [{ \"filedAt\": { \"order\": \"desc\" }}]\n",
    "            }\n",
    "    response = req.post(base_url, json=payload, headers={'Authorization': SEC_API_KEY})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        comp_tenk_filing = response.json()\n",
    "        return comp_tenk_filing\n",
    "    else:\n",
    "        raise Exception(f'Error fetching Company 10 k filings. Status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_get_filings(tkr):\n",
    "    try:\n",
    "        time.sleep(sleep_time + random.uniform(0, 0.5))\n",
    "        return get_comp_sec(tkr)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {tkr}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_filings_info = [safe_get_filings(tkr) for tkr in company_tickers]\n",
    "print(company_filings_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_info = []\n",
    "for comp in company_filings_info:\n",
    "    if comp is not None:\n",
    "        for filing in comp['filings']:\n",
    "            company_info.append({\n",
    "                \"company\": filing[\"companyName\"],\n",
    "                \"filing_date\": filing[\"filedAt\"],\n",
    "                \"link\": filing[\"linkToFilingDetails\"],\n",
    "                \"text\": filing[\"linkToTxt\"],\n",
    "            })\n",
    "    else:\n",
    "        print(\"No filings found.\")\n",
    "print(company_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beautiful_soup(url, company, date):\n",
    "    \"\"\"\n",
    "    Get the soup object from the url.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The url of the filing.\n",
    "    \n",
    "    Returns:\n",
    "        object: soup object.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "    'User-Agent': EMAIL\n",
    "    }\n",
    "    response = req.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'xml')\n",
    "        # write the soup to a file\n",
    "        date = date.replace(\"-\", \"_\")\n",
    "        company = company.replace(\" \", \"_\")\n",
    "        if not os.path.exists(\"financialreport\"):\n",
    "            os.makedirs(\"financialreport\")\n",
    "        with open(f'financialreport/{company}_{date}_raw.txt', 'w') as f:\n",
    "            f.write(soup.get_text())\n",
    "        return soup.get_text()\n",
    "    else:\n",
    "        raise Exception(f'Error fetching filing. Status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sentiment analyses of financial report using ProsusAI/finbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define function to get overall sentiment\n",
    "def overall_sentiment(text):\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return {\n",
    "            \"overall_sentiment\": \"error\",\n",
    "            \"average_score\": None\n",
    "        }\n",
    "    \n",
    "    # Break the text into chunks (basic method)\n",
    "    chunks = [text[i:i + 1000] for i in range(0, len(text), 1000)]\n",
    "\n",
    "    #make empty list for sentiment score\n",
    "    sentiment_score = []\n",
    "\n",
    "    # Analyze sentiment\n",
    "    for chunk in chunks:\n",
    "        result = sentiment_analyzer(chunk[:512])\n",
    "        sentiment_score.append(result[0][\"score\"])\n",
    "    \n",
    "    # Calculate the average score\n",
    "    average_score = np.mean(sentiment_score)\n",
    "    \n",
    "    # Determine the overall sentiment based on the average score\n",
    "    if average_score >= 0.75:\n",
    "        overall_sentiment = 'positive'\n",
    "    elif average_score <= 0.25:\n",
    "        overall_sentiment = 'negative'\n",
    "    else:\n",
    "        overall_sentiment = 'neutral'\n",
    "    \n",
    "    return {\n",
    "           \"overall_sentiment\": overall_sentiment,\n",
    "           \"average_score\": average_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a LLM that model that iterate through the list of pdf files to analyze, syntesize, and summarize the pdf pages content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a dataframe to count how many times a financial_keywords appears in the filing and add to the dataframe\n",
    "def display_financial_keywords_count_df(text, keywords):\n",
    "    \"\"\"\n",
    "    Count the occurrences of financial keywords in the text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to analyze.\n",
    "        keywords (list): A list of financial keywords to count.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with keyword counts.\n",
    "    \"\"\"\n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "    keyword_counts = {keyword: word_tokens.count(keyword.lower()) for keyword in keywords}\n",
    "    financial_keyword_df = pd.DataFrame(keyword_counts.items(), columns=['Keyword', 'Count'])\n",
    "    display(financial_keyword_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_financial_report(text, num_sentences=5):\n",
    "    \"\"\"\n",
    "    Summarizes a financial report using a frequency-based approach with NLTK.\n",
    "\n",
    "    Args:\n",
    "        text (str): The financial report text.\n",
    "        num_sentences (int): The desired number of sentences in the summary.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated summary.\n",
    "    \"\"\"\n",
    "    financial_keywords = [\"revenue\", \"profit\", \"income\", \"loss\", \"assets\", \"liabilities\",\n",
    "                            \"equity\", \"cash flow\", \"margin\", \"earnings\", \"sales\", \"billion\",\n",
    "                            \"million\", \"thousand\", \"%\", \"increase\", \"decrease\", \"growth\"]\n",
    "\n",
    "    # 1. Tokenization\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # 2. Remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped_words = [w.translate(table) for w in words]\n",
    "\n",
    "    # 3. Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [w for w in stripped_words if w.isalpha() and w not in stop_words]\n",
    "\n",
    "    # 4. Word frequency calculation\n",
    "    word_frequency = {}\n",
    "    for word in filtered_words:\n",
    "        word_frequency[word] = word_frequency.get(word, 0) + 1\n",
    "\n",
    "    # Calculate weighted frequencies (optional, can help emphasize important words)\n",
    "    max_frequency = max(word_frequency.values()) if word_frequency else 1\n",
    "    weighted_frequency = {word: freq / max_frequency for word, freq in word_frequency.items()}\n",
    "\n",
    "    # 5. Sentence tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # 6. Sentence scoring\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            word = word.translate(table)\n",
    "            if word in weighted_frequency:\n",
    "                score_multiplier = 1.5 if word in financial_keywords else 1.0\n",
    "                sentence_scores[sentence] = sentence_scores.get(sentence, 0) + weighted_frequency[word] * score_multiplier\n",
    "\n",
    "    # 7. Summary generation\n",
    "    sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
    "    summary_sentences = sorted_sentences[:num_sentences]\n",
    "    summary = \" \".join(summary_sentences)\n",
    "    display_financial_keywords_count_df(summary, financial_keywords)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summary_to_file(company, date, summary):\n",
    "    if not os.path.exists(\"summaries\"):\n",
    "        os.makedirs(\"summaries\")\n",
    "    with open(f\"summaries/{company}_{date}_summary.txt\", \"w\") as f:\n",
    "        f.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_company_data = []\n",
    "for info in company_info:\n",
    "    link = info['link']\n",
    "    date = info['filing_date']\n",
    "    company = info['company']\n",
    "    filing_text = beautiful_soup(link, company, date)\n",
    "    # save_summary_to_file(company, date, summarize_financial_report(filing_text, num_sentences=3))\n",
    "    print(f\"Filing text for {company} on {date} saved.\")\n",
    "    # Append the filing text to the data list\n",
    "    detail_company_data.append({\n",
    "        \"company\": company,\n",
    "        \"filing_date\": date,\n",
    "        \"filing_text\": filing_text\n",
    "    })\n",
    "print(overall_sentiment(detail_company_data[0]['filing_text']))\n",
    "# Create a DataFrame from the data list and overall sentiment\n",
    "data_df = pd.DataFrame(detail_company_data)\n",
    "sentiment_results = data_df[\"filing_text\"].apply(overall_sentiment)\n",
    "data_df[\"sentiment\"] = sentiment_results.apply(lambda x: x[\"overall_sentiment\"]) \n",
    "data_df[\"sentiment_score\"] = sentiment_results.apply(lambda x: x[\"average_score\"])\n",
    "data_df = data_df.drop(columns=[\"filing_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Sentiment analysis dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run LLM app using OpenAi Langchain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run OpenaiLangchain.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
